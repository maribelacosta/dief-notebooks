{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Computing Diefficiency Metrics with the Dief R Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing the Dief R Package from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have not installed the \"devtools\" package.\n",
    "install.packages(\"devtools\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the dief package from GitHub repository.\n",
    "library(\"devtools\")\n",
    "devtools::install_github(\"maribelacosta/dief\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dief R package.\n",
    "library(\"dief\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the diefficiency metrics <b><i>dief@t</i></b> and <b><i>dief@k</i></b>, we need the answer trace produced by the SPARQL query engines when executing queries. <br />\n",
    "\n",
    "<b>Answer traces</b> record the exact point in time when an engine produces an answer when executing a query. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we executed SPARQL queries using three different configurations of the <a href=\"http://people.aifb.kit.edu/mac/nlde/\">nLDE engine</a>: `Selective`,  `NotAdaptive`,  `Random`.<br\\> \n",
    "\n",
    "The resulting answer trace for each query execution is stored in a CSV file available at <a href=\"https://ndownloader.figshare.com/files/8955184\">FigShare</a> (137.06 MB). The structure of the CSV file is as follows:\n",
    "<ul>\n",
    "<li><b>`query`</b>: id of the query executed. Example: 'Q9.sparql'</li>\n",
    "<li><b>`approach`</b>: name of the approach (or engine) used to execute the query. </li>\n",
    "<li><b>`tuple`</b>: the value `i` indicates that this row corresponds to the ith answer produced by `approach` when executing `query`. </li>\n",
    "<li><b>`time`</b>: elapsed time (in seconds) since `approach` started the execution of `query` until the answer `i` is produced. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download the answer trace file and read it in the variable `traces`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the answer trace file with the query traces from FigShare. \n",
    "# Store the downloaded file in '/tmp/traces.csv'.\n",
    "traces_file <- \"/tmp/traces.csv\"\n",
    "download.file(\"https://ndownloader.figshare.com/files/8955184\", traces_file)\n",
    "traces <- read.csv(traces_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>query</th><th scope=col>approach</th><th scope=col>tuple</th><th scope=col>time</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Q11.sparql</td><td>Selective </td><td>1         </td><td>8.670243  </td></tr>\n",
       "\t<tr><td>Q11.sparql</td><td>Selective </td><td>2         </td><td>8.877625  </td></tr>\n",
       "\t<tr><td>Q11.sparql</td><td>Selective </td><td>3         </td><td>8.878194  </td></tr>\n",
       "\t<tr><td>Q11.sparql</td><td>Selective </td><td>4         </td><td>9.129733  </td></tr>\n",
       "\t<tr><td>Q11.sparql</td><td>Selective </td><td>5         </td><td>9.132296  </td></tr>\n",
       "\t<tr><td>Q11.sparql</td><td>Selective </td><td>6         </td><td>9.133606  </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llll}\n",
       " query & approach & tuple & time\\\\\n",
       "\\hline\n",
       "\t Q11.sparql & Selective  & 1          & 8.670243  \\\\\n",
       "\t Q11.sparql & Selective  & 2          & 8.877625  \\\\\n",
       "\t Q11.sparql & Selective  & 3          & 8.878194  \\\\\n",
       "\t Q11.sparql & Selective  & 4          & 9.129733  \\\\\n",
       "\t Q11.sparql & Selective  & 5          & 9.132296  \\\\\n",
       "\t Q11.sparql & Selective  & 6          & 9.133606  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "query | approach | tuple | time | \n",
       "|---|---|---|---|---|---|\n",
       "| Q11.sparql | Selective  | 1          | 8.670243   | \n",
       "| Q11.sparql | Selective  | 2          | 8.877625   | \n",
       "| Q11.sparql | Selective  | 3          | 8.878194   | \n",
       "| Q11.sparql | Selective  | 4          | 9.129733   | \n",
       "| Q11.sparql | Selective  | 5          | 9.132296   | \n",
       "| Q11.sparql | Selective  | 6          | 9.133606   | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  query      approach  tuple time    \n",
       "1 Q11.sparql Selective 1     8.670243\n",
       "2 Q11.sparql Selective 2     8.877625\n",
       "3 Q11.sparql Selective 3     8.878194\n",
       "4 Q11.sparql Selective 4     9.129733\n",
       "5 Q11.sparql Selective 5     9.132296\n",
       "6 Q11.sparql Selective 6     9.133606"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the trace file.\n",
    "head(traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, in this tutorial we are going to compare the performance of the nLDE engine using the metrics <i>dief@t</i> and <i>dief@k</i> as well as <b>conventional metrics</b> used in the query processing literature, such as: <i>execution time</i>, <i>time for the first tuple</i>, and <i>number of answers produced</i>.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we measured the performance of the nLDE engine using conventional metrics. The results are available at another CSV file in <a href=\"https://ndownloader.figshare.com/files/8955187\">FigShare</a> (2.86 KB). The structure of this CSV file is as follows: \n",
    "<ul>\n",
    "<li><b>`query`</b>: id of the query executed. Example: 'Q9.sparql'</li>\n",
    "<li><b>`approach`</b>: name of the approach (or engine) used to execute the query. </li>\n",
    "<li><b>`tfft`</b>: time (in seconds) required by `approach` to produce the first tuple when executing `query`. </li>\n",
    "<li><b>`totaltime`</b>: elapsed time (in seconds) since `approach` started the execution of `query` until the last answer  of `query` is produced. </li>\n",
    "<li><b>`comp`</b>: number of answers produced by `approach` when executing `query`. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download the result of the other metrics (execution time, etc.) from FigShare.\n",
    "# Store the downloaded file in '/tmp/metrics.csv'.\n",
    "metrics_file <- \"/tmp/metrics.csv\"\n",
    "download.file(\"https://ndownloader.figshare.com/files/8955187\", metrics_file)\n",
    "metrics <- read.csv(metrics_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>query</th><th scope=col>approach</th><th scope=col>tfft</th><th scope=col>totaltime</th><th scope=col>comp</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>Q11.sparql </td><td>NotAdaptive</td><td>  5.886026 </td><td>300.02924  </td><td> 1287      </td></tr>\n",
       "\t<tr><td>Q12.sparql </td><td>NotAdaptive</td><td> 13.681366 </td><td> 74.35181  </td><td> 4834      </td></tr>\n",
       "\t<tr><td>Q13.sparql </td><td>NotAdaptive</td><td> 23.121218 </td><td>300.01434  </td><td>14371      </td></tr>\n",
       "\t<tr><td>Q14.sparql </td><td>NotAdaptive</td><td>162.549945 </td><td>300.01342  </td><td>    3      </td></tr>\n",
       "\t<tr><td>Q15.sparql </td><td>NotAdaptive</td><td>  6.423510 </td><td>300.03944  </td><td> 3430      </td></tr>\n",
       "\t<tr><td>Q16.sparql </td><td>NotAdaptive</td><td> 28.438493 </td><td>300.32796  </td><td>13940      </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllll}\n",
       " query & approach & tfft & totaltime & comp\\\\\n",
       "\\hline\n",
       "\t Q11.sparql  & NotAdaptive &   5.886026  & 300.02924   &  1287      \\\\\n",
       "\t Q12.sparql  & NotAdaptive &  13.681366  &  74.35181   &  4834      \\\\\n",
       "\t Q13.sparql  & NotAdaptive &  23.121218  & 300.01434   & 14371      \\\\\n",
       "\t Q14.sparql  & NotAdaptive & 162.549945  & 300.01342   &     3      \\\\\n",
       "\t Q15.sparql  & NotAdaptive &   6.423510  & 300.03944   &  3430      \\\\\n",
       "\t Q16.sparql  & NotAdaptive &  28.438493  & 300.32796   & 13940      \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "query | approach | tfft | totaltime | comp | \n",
       "|---|---|---|---|---|---|\n",
       "| Q11.sparql  | NotAdaptive |   5.886026  | 300.02924   |  1287       | \n",
       "| Q12.sparql  | NotAdaptive |  13.681366  |  74.35181   |  4834       | \n",
       "| Q13.sparql  | NotAdaptive |  23.121218  | 300.01434   | 14371       | \n",
       "| Q14.sparql  | NotAdaptive | 162.549945  | 300.01342   |     3       | \n",
       "| Q15.sparql  | NotAdaptive |   6.423510  | 300.03944   |  3430       | \n",
       "| Q16.sparql  | NotAdaptive |  28.438493  | 300.32796   | 13940       | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  query      approach    tfft       totaltime comp \n",
       "1 Q11.sparql NotAdaptive   5.886026 300.02924  1287\n",
       "2 Q12.sparql NotAdaptive  13.681366  74.35181  4834\n",
       "3 Q13.sparql NotAdaptive  23.121218 300.01434 14371\n",
       "4 Q14.sparql NotAdaptive 162.549945 300.01342     3\n",
       "5 Q15.sparql NotAdaptive   6.423510 300.03944  3430\n",
       "6 Q16.sparql NotAdaptive  28.438493 300.32796 13940"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the metric file.\n",
    "head(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Answer Traces of Continuos SPARQL Query Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To measure the diefficiency of approaches, the metrics <b><i>dief@t</i></b> and <b><i>dief@k</i></b> compute the area under the curve of <b>answer traces</b>. Answer traces record the points in time when an approach produces an answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `dief` R package, it is possible to plot the answer trace of a SPARQL engine when executing a query using the `dief::plotAnswerTrace` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in eval(expr, envir, enclos): could not find function \"plotAnswerTrace\"\n",
     "output_type": "error",
     "traceback": [
      "Error in eval(expr, envir, enclos): could not find function \"plotAnswerTrace\"\nTraceback:\n"
     ]
    }
   ],
   "source": [
    "# Plot the answer trace recorded in `traces` for query `Q9.sparql`\n",
    "plotAnswerTrace(traces, \"Q9.sparql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: For `Q9.sparql`, we obseve that the answer trace of nLDE `Not Adaptive` (red line) surpasses the answer traces of the other approaches. This indicates that nLDE `Not Adaptive` continuously produces more answers than the other approaches.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Measuring Performance with dief@t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric <b><i>dief@t</i></b> measures the diefficiency of an engine in the first <i>t</i> time units of query execution. Intuitively, approaches that produce answers at a higher rate in a certain period of time are more efficient. <b><i>dieft@t interpretation: Higher is better.</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dieft` function computes the <b><i>dief@t</i></b> metric as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dief@t of the approches recorded in `traces` when executing `Q9.sparql' \n",
    "# until the time unit 16 (in seconds).\n",
    "dieft(traces, \"Q9.sparql\", 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dief@t of the approches recorded in `traces` when executing `Q9.sparql' \n",
    "# until the  time unit when the fastest approach finalizes its execution.\n",
    "dieft(traces, \"Q9.sparql\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing Experiment 1: Comparing Dief@t with Other Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "In this experiment, we compare the performance of the three variants of nLDE using conventional metrics from the query processing literature and <b><i>dief@t</i></b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `experiment1` computes all the results reported in Experiment 1 at [1].\n",
    "\n",
    "[1] Maribel Acosta, Maria-Esther Vidal, York Sure-Vetter. Diefficiency Metrics: Measuring the Continuous Efficiency of Query Processing Approaches. In Proceedings of the International Semantic Web Conference, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Reproduce Experiment 1 using the files `traces_file` and `metrics_file`.\n",
    "exp1 <- experiment1(traces_file, metrics_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the results for query `Q9.sparql`. \n",
    "# `throughput` is computed as comp/totaltime.\n",
    "# `invtfft` is computed as 1/tfft.\n",
    "# `invtotaltime` is computed as 1/totaltime.\n",
    "subset(exp1, query==\"Q9.sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot radar chart to compare the performance of the approaches with dief@t and other metrics.\n",
    "# Plot interpreation: Higher is better. \n",
    "plotExperiment1Query(exp1, \"Q9.sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the results of Experiment 1 for all the queries in the benchmark use the following function.\n",
    "plotExperiment1(exp1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: For `Q9.sparql`, the values of the metrics from the literature indicate that the three nLDE variants are competitive approaches. Yet, <i>dief@t</i> allows for uncovering that nLDE `Not Adaptive` is able to continuously produce answers at a faster rate than the other approaches for this query untile the fastest approach finilaizes its execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Performance with dief@k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metric <b><i>dief@k</i></b> measures the diefficiency of an engine while producing the first <i>k</i> answers when executing a query.  Intuitively, approaches that require a shorter period of time to produced a certain number of answers are more efficient. <b><i>dief@k interpretation: Lower is better.</i></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `diefk` and `diefk2` functions compute the <b><i>dief@k</i></b> metric as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dief@k of the approches recorded in `traces` when executing `Q9.sparql' \n",
    "# and producing the first 2,000 answers.\n",
    "diefk(traces, \"Q9.sparql\", 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dief@k of the approches recorded in `traces` when executing `Q9.sparql' \n",
    "# and producing the first k answers, where k is the minimum of the total of answers produced among all the approaches.\n",
    "diefk(traces, \"Q9.sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dief@k of the approches recorded in `traces` when executing `Q9.sparql' \n",
    "# and producing 50% of the answers. \n",
    "diefk2(traces, \"Q9.sparql\", 0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproducing Experiment 2: Measuring dief@t at Different Answer Completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we compare the performance of the three variants of nLDE when producing different answer completeness percentages (25%, 50%, 75%, 100%) using  <b></i>dief@k</i></b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `experiment2` computes all the results reported in Experiment 2 at [1].\n",
    "\n",
    "[1] Maribel Acosta, Maria-Esther Vidal, York Sure-Vetter. Diefficiency Metrics: Measuring the Continuous Efficiency of Query Processing Approaches. In Proceedings of the International Semantic Web Conference, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Reproduce Experiment 2 using the file `traces_file`.\n",
    "exp2 <- experiment2(traces_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot radar chart to compare the performance of the approaches with dief@k \n",
    "# at different answer completeness percentages (25%, 50%, 75%, 100%). \n",
    "# Plot interpreation: Lower is better. \n",
    "plotExperiment2Query(exp2, \"Q9.sparql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot the results of Experiment 2 for all the queries in the benchmark use the following function.\n",
    "plotExperiment2(exp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Conclusion</b>: For `Q9.sparql`, the variants nLDE `Random` and `Selective` exihibit similar values of <i>dief@k</i> while producing the first 25% of the answers. However, when looking at <i>dief@t</i> at 100%, we can conclude that once nLDE `Selective` starts producing answers, it produces all the answers at a faster rate. This can be observed in the answer trace plot, where the trace for nLDE `Selective` (blue line) has a higher slope over time than the other approaches. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
